{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Art of Training Neural Networks with Keras\n",
    "\n",
    "<br />\n",
    "\n",
    "## Tools in the Deep Learning Ecosystem\n",
    "\n",
    "<img src=\"img/deep_learning_ecosystem.jpeg\" />\n",
    "\n",
    "<br />\n",
    "\n",
    "* Hardware GPU vs CPU: At the core hardware level, we have CPUs and GPUs executing instructions. \n",
    "\n",
    "    <br />\n",
    "\n",
    "    * A CPU is able to execute large, sequential instructions but can only execute a small number of instructions parallelly\n",
    "    \n",
    "    <br />\n",
    "    \n",
    "    * A GPU can execute hundreds of small instructions parallelly\n",
    "    \n",
    "<img src = 'img/cpu_vs_gpu.png' />\n",
    "\n",
    "* For deep learning where we have to do a bunch of linear algebraic computations parallelly, GPUs are exponentially faster than CPUs\n",
    "\n",
    "<br />\n",
    "\n",
    "* Compute Frameworks such as BLAS and CUDA help routine computations to be optimised for the specific processor instruction set for accelerated compute\n",
    "\n",
    "    <br />\n",
    "    \n",
    "    * Basic Linear Algebra Subprograms are a bunch of routines that specify the low level routines for many linear algebraic operations. R, numpy, Matlab use BLAS to accelerate linear algebra operations. Ex: Intel's implementation of BLAS is known as Intel Math Kernel Library (MKL)\n",
    "    \n",
    "    <br />\n",
    "    \n",
    "    * CUDA is a framework created by NVIDIA that helps programmers write software to perform general purpose computing tasks on GPUs, most of the deep learning libraries use CUDA\n",
    "\n",
    "<br />\n",
    "\n",
    "* Libraries with support for Autodifferentiation to help in gradient computation for stacked layers were developed such as Theano, Tensorflow, CNTK and PyTorch. \n",
    "\n",
    "* These libraries are operations level (dot product, etc.) and the code is low level and hard to write for quick prototyping of new networks\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/deep_learning_frameworks.png' width='550px'/>\n",
    "\n",
    "<br />\n",
    "\n",
    "* That is wehre Keras comes into picture, it is a high level library with the abstraction at the __layer__ level. It was built as an abstraction to Theano and later support was added to Tensorflow and CNTK\n",
    "\n",
    "* So you can write code in Keras that can run on any of these three deep learning library __backends__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Building Blocks of Neural Networks\n",
    "\n",
    "### Layers, Data and Learning Representations\n",
    "\n",
    "<br />\n",
    "\n",
    "* __Layers__: logically grouped operations in a neural network, the parameters for the operations in the layer learn to generate the best features to predict the target\n",
    " \n",
    "<img src='img/nn_layers.jpeg' width='400px'/>\n",
    "\n",
    "<img src='img/nn_layer_operations.png' width='650px'/>\n",
    "\n",
    "<br />\n",
    "\n",
    "* The network needs to have __input data__ and corresponding __targets (y)__\n",
    "\n",
    "<br />\n",
    "\n",
    "* In traditional machine learning we see that changing the representation of the data (kernel trick, etc.) helps ease the process of learning from data\n",
    "\n",
    "<br />\n",
    "\n",
    "* __Activation function__ adds that non linearity and in combination with weights (parameters) of a layer, the network learns better representations of the data at each layer\n",
    "\n",
    "<br />\n",
    "\n",
    "### So basically, each layer takes input as data and spits out transformed data as output, simple as that. Now, let's dive into the details\n",
    "\n",
    "<img src='img/learning_representations_mlp.jpg' />\n",
    "\n",
    "<br />\n",
    "\n",
    "* The goal of training neural networks is to find these perfect representation of data, which we get by \"learning\" the right weights\n",
    "\n",
    "<img src='img/learning_weights.jpg' />\n",
    "\n",
    "<br />\n",
    " \n",
    "* The loss function, which defines the feedback signal used for learning helps guage __how different are the targets and the predicted targets__\n",
    "\n",
    "<img src='img/loss_function.jpg' />\n",
    "\n",
    "<br />\n",
    " \n",
    "* The optimizer, based on the feedback signal from the loss function changes the parameters / weights of the network to help make the predictions as close to the target as possible (minimizing the loss function)\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/building_blocks_of_neural_networks.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras Interface\n",
    "\n",
    "<br />\n",
    "\n",
    "* There are two major ways to define and run neural netwroks using the Keras API\n",
    "    \n",
    "    <br />\n",
    "    \n",
    "    * Sequential API\n",
    "    \n",
    "    <br />\n",
    "    \n",
    "    * Functional API\n",
    "\n",
    "<br />\n",
    "\n",
    "### The Sequential API\n",
    "\n",
    "* The sequential api allows us to __quickly stack layers__ and build networks\n",
    "\n",
    "<img src=\"img/keras_interface.jpg\" width='550px'/>\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/keras_sequential_api.jpg' />\n",
    "\n",
    "<br />\n",
    "\n",
    "### The Functional API\n",
    "\n",
    "* The functional api allows us to build complex graph networks, we can kee chaining the the layers as functions and finally the `Model(inputs, outputs)` class connects all the various inputs and outputs\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src=\"img/functional_api_bimodal_network.png\" width='450px'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are different categories of layers in Keras, the most commonly used ones are \"Dense Layers\", \"Convolution Layers\", \"Recurrent Layers\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(units = 32, activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's break down the above layer that we just built using the Dense class from keras\n",
    "\n",
    "<br />\n",
    "\n",
    "* The OUTPUT from the above layer can be given by `sigmoid ( dot ( W, INPUT ) + b )`, in the first round of training the W (weights) are randomly \"initialized\"\n",
    "\n",
    "<br />\n",
    "\n",
    "* Every input to the Dense layer (also known as fully connected) is connected to every unit in the hidden layer, as shown in the figure below\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src ='img/fc_dense_layers_keras.jpg' />\n",
    "\n",
    "<br />\n",
    "\n",
    "* We will see many more categories and types of __Layers__ in keras, the __\"Dense\"__ layer is just one such class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the layers together : The Keras Sequential API\n",
    "\n",
    "<br />\n",
    "\n",
    "* The keras sequential api enables us to build common yet complex neural network architectures flexibly\n",
    "\n",
    "<br />\n",
    "\n",
    "* Objects of the Keras sequential class, can have multiple neural network layers stacked on top of one another\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/keras_sequential_api.jpg' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* You can create a Keras sequential model by passing in a list of layers to the Sequential object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,), activation = 'sigmoid'),\n",
    "    Dense(10, activation = 'sigmoid'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But soon this becomes a problem to add more layers in a single list, so the \"add\" method on the sequential class object can add layers sequentially to the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network = Sequential()\n",
    "\n",
    "neural_network.add(Dense(32, input_dim=784, activation = 'sigmoid'))\n",
    "\n",
    "neural_network.add(Dense(10, activation = 'sigmoid'))\n",
    "\n",
    "neural_network.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The summary method on the neural network object gives us basic information about the structure of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 25,461\n",
      "Trainable params: 25,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* That's all! Keras is that simple, initialize an object of the Sequential class, use the add method on that object to add layers to the network sequentially, but wait what about the loss function? what about the optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining, Compiling and Running a Neural Network in Keras\n",
    "\n",
    "<br />\n",
    "\n",
    "### The process of learning in a Neural Network\n",
    "\n",
    "<br />\n",
    "\n",
    "### Loss Score\n",
    "\n",
    "<br />\n",
    "\n",
    "* The loss score is feedback signal that says how far is the output of your network compared to the ground truth\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/loss_function.jpg' />\n",
    "\n",
    "<br />\n",
    "\n",
    "* Two such loss scores that we use quite frequently are :\n",
    "    \n",
    "    1) Binary Cross-Entropy: For Two-Class Classification problems\n",
    "    \n",
    "    2) Mean Squared Error: For Regression problems\n",
    "    \n",
    "<br />\n",
    "\n",
    "* We have already come across mean squared error before, so let's dive deeper into Binary Cross Entropy\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln p + (1-y ) \\ln (1-p) \\right]\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "<br />\n",
    "\n",
    "* In the above equation, p is the output of the network, n is the total number of samples in the training data, the sum is over all training inputs, x, and y is the corresponding desired output\n",
    "\n",
    "<br />\n",
    "\n",
    "* Below, we see the value of the cross entropy (sometimes referred to as the log loss) changing with the predicted probability, we can see that the value of the loss for a prediction above 0.5 significantly drops and this helps the network converge much faster than using a traditional mean squared error\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/binary_cross_entropy.png' width='300px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "<br />\n",
    "\n",
    "* An optimizer is an algorithm that uses the feedback signal from the loss function, to actually update the weights so that the output from the network gets closer to the ground truth. The first optimizer that we use is Stochastic Gradient Descent (SGD), we will slowly come across many more optimizers\n",
    "\n",
    "<br />\n",
    "\n",
    "* We can import Classes from the optimizers module of keras and customize the specific optimizer to our liking\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/building_blocks_of_neural_networks.jpg' /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "customized_optimizer = SGD(lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We already know how learning rate can effect convergence, the graph below provides a decent intuition, hence having the flexibility to change the learning rate is very important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/learning_rate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the neural network ( loss function + optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_network.compile(loss = 'binary_crossentropy', optimizer = customized_optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see from the compile step above we need to specify the loss function, optimization algorithm and we can also mention the metrics that we want to monitor while training the neural network\n",
    "\n",
    "<br />\n",
    "\n",
    "* Also, please __note__ that the optimizer argument can either be a string or a customizable object from the optimizers module in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So are we done? what about learning the weights? \n",
    "\n",
    "<br />\n",
    "\n",
    "* Training the network in Keras is also very simple, we call the `.fit()` method and pass in the arguments\n",
    "\n",
    "<br />\n",
    "\n",
    "* Some important terms for training neural networks are epochs, batch_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NOTE : Don't run the following line of code as we do not yet have X_train and y_train\n",
    "\n",
    "neural_network.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An __Epoch__ is when an ENTIRE dataset is passed forward and backward through the neural network only once.\n",
    "\n",
    "<br />\n",
    "\n",
    "* Since most of the times an epoch is too large to fit in memory, we divide the data into batches and compute the gradient on batches for each forward and backward pass\n",
    "\n",
    "<br />\n",
    "\n",
    "* __Batch size__ is the number of samples that are going to be propagated through the network.\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/learning_rate_epochs_rel.png' width='400px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures for Basic ML Tasks\n",
    "\n",
    "<br />\n",
    "\n",
    "<img src='img/nn_for_basic_ml_tasks.jpg' width='800px'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
